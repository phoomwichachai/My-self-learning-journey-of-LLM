{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JhzNfnfiHgP",
        "outputId": "cb8d8950-a08e-44b7-9019-0e7973a3c786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-04 03:39:14--  https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‚Äòwiki_sample.txt‚Äô\n",
            "\n",
            "wiki_sample.txt     100%[===================>]   6.19M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-12-04 03:39:14 (65.9 MB/s) - ‚Äòwiki_sample.txt‚Äô saved [6488666/6488666]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wiki_sample.txt https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt\n",
        "# !wget -O thai_sample.txt https://th.wikipedia.org/wiki/%E0%B8%9B%E0%B8%B1%E0%B8%8D%E0%B8%8D%E0%B8%B2%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%94%E0%B8%B4%E0%B8%A9%E0%B8%90%E0%B9%8C\n",
        "# (or another small wiki-like article)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jsg7abbDiIth"
      },
      "outputs": [],
      "source": [
        "# !wget -O attn_paper.pdf https://arxiv.org/pdf/1706.03762.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "f02eb1f1a3fd4084bb73855e171ef1c0",
            "03058aaf087b4d9e80112629f83da4f0",
            "01c7f50e59cb4e25bde67080aa100544",
            "b9b435728c1d4e0da388cce2a57ed0b7",
            "aadd71386f7942b4bb98c7d4516122b0",
            "25ed4ba7f75d48d0b0fe0ca1ee2ce910",
            "91ae609281274d8ca9cb394a37ffff15",
            "224610ccc6f240ae9f3d04a03c904d85",
            "b97fade6aaa94fefa3241f998d3e880b",
            "65ec0edee62c49588af67157c7aa9bd5",
            "c77d7ad514df437b89d9294bdfd768c0",
            "890286cf93164347b8c14dbdbc43a7ef",
            "295765149a24496a9408090b1e247ead",
            "cdb7eeba3e7944d7be426b74e7bbdbb0",
            "f35779a8343c43e6b6547979b1d7a41f",
            "dbe4b92a65d3419ba22fcdce9c0ff179",
            "b87cce326878451dac31913788cc6d4c"
          ]
        },
        "id": "ETkwbIRaushM",
        "outputId": "8156da19-b9ce-42c8-81ca-ff4dc159d103"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f02eb1f1a3fd4084bb73855e171ef1c0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6-mhGFIQiLZ5"
      },
      "outputs": [],
      "source": [
        "# ‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "!pip install --quiet transformers accelerate sentencepiece huggingface_hub \\\n",
        "    sentence-transformers faiss-cpu PyPDF2 nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGTAyF_ni2GR",
        "outputId": "bf85e418-c1a0-40fc-987f-b9782ae386d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os, json, re, math, numpy as np\n",
        "import faiss, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHZrqwHui5h4",
        "outputId": "ecc5118a-ac68-42b6-e562-dd7e4748a529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell C loaded successfully. PDF + TXT + English + Thai chunkers ready.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# ---------- TXT Reader ----------\n",
        "def read_text(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# ---------- PDF Reader ----------\n",
        "def read_pdf(path):\n",
        "    reader = PdfReader(path)\n",
        "    pages = []\n",
        "    for p in reader.pages:\n",
        "        try:\n",
        "            pages.append(p.extract_text() or \"\")\n",
        "        except:\n",
        "            pages.append(\"\")\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "# ---------- NLTK Sentence Splitter (English) ----------\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def ensure_nltk():\n",
        "    # punkt\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "    # punkt_tab (NLTK >=3.8)\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt_tab')\n",
        "\n",
        "def text_to_chunks_by_sentence(text, max_words=300, overlap_words=60):\n",
        "    ensure_nltk()\n",
        "    sents = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    cur, cur_len = [], 0\n",
        "    for s in sents:\n",
        "        w = s.split()\n",
        "        if cur_len + len(w) > max_words and cur:\n",
        "            chunks.append(\" \".join(cur).strip())\n",
        "            # overlap\n",
        "            if overlap_words > 0:\n",
        "                tail = \" \".join(\" \".join(cur).split()[-overlap_words:])\n",
        "                cur = tail.split()\n",
        "                cur_len = len(cur)\n",
        "            else:\n",
        "                cur, cur_len = [], 0\n",
        "        cur.extend(w)\n",
        "        cur_len += len(w)\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur).strip())\n",
        "    return chunks\n",
        "\n",
        "# ---------- Lightweight splitter (Thai / fallback) ----------\n",
        "def split_sentences_light(text):\n",
        "    # no lookbehind ‚Üí safe for all Python versions\n",
        "    return [s.strip() for s in re.split(r'[.!?]\\s+|\\n+', text) if s.strip()]\n",
        "\n",
        "def text_to_chunks_light(text, max_words=250, overlap_words=50):\n",
        "    sents = split_sentences_light(text)\n",
        "    chunks = []\n",
        "    cur, cur_len = [], 0\n",
        "    for s in sents:\n",
        "        w = s.split()\n",
        "        if cur_len + len(w) > max_words and cur:\n",
        "            chunks.append(\" \".join(cur).strip())\n",
        "            if overlap_words > 0:\n",
        "                tail = \" \".join(\" \".join(cur).split()[-overlap_words:])\n",
        "                cur = tail.split()\n",
        "                cur_len = len(cur)\n",
        "            else:\n",
        "                cur, cur_len = [], 0\n",
        "        cur.extend(w)\n",
        "        cur_len += len(w)\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur).strip())\n",
        "    return chunks\n",
        "\n",
        "print(\"Cell C loaded successfully. PDF + TXT + English + Thai chunkers ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24l3A3qCjH9o",
        "outputId": "18bb27c9-6bd5-4a5a-8ab2-b6d3c3ac4028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English text length: 6488666\n",
            "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
            "by Sir Arthur Conan Doyle\n",
            "(#15 in our series by Sir Arthur Conan Doyle)\n",
            "\n",
            "Copyright laws are changing all over the world. Be sure to check the\n",
            "copyright laws for your country before downloading or redistributing\n",
            "this or any other Project Gutenberg eBook.\n",
            "\n",
            "This header should be the first thing seen when viewing this Project\n",
            "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
            "header without written permission.\n",
            "\n",
            "Please read the \"legal small print,\" and other information about the\n",
            "eBook and Project Gutenberg at the bottom of this file.  Included is\n",
            "important information about your specific rights and restrictions in\n",
            "how the file may be used.  You can also find out about how to make a\n",
            "donation to Project Gutenberg,\n",
            "\n",
            "---\n",
            "Thai text length: 59518\n",
            "0 \n",
            " \n",
            " Generative AI Governance  \n",
            "Guideline for Organizations  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "1 Generative AI Governance  \n",
            "Guideline for Organizations  \n",
            " \n",
            "‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç  \n",
            "‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£           \n",
            "Executive Summary          4 \n",
            "01 ‡∏ó ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Generative AI      \n",
            "What is Generative AI ?        7 \n",
            "1.1 ‡∏Ñ ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≥‡∏°             9 \n",
            "1.2 ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏´‡∏°‡∏≥‡∏¢ ‡∏Ç‡∏≠‡∏á Generative AI        10 \n",
            "1.3 ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏™‡∏≥‡∏°‡∏≥‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á Generative  AI       12 \n",
            "02 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏à ‡∏≤‡∏Å‡∏±‡∏î ‡∏Ç‡∏≠‡∏á Generative AI  \n",
            "Benefits and Limitations of Generative AI     13 \n",
            "2.1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≥‡∏Å ‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI      14 \n",
            "2.2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≥‡∏á‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI       16 \n",
            "2.3 ‡∏Ç‡πâ‡∏≠‡∏à ‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Generative AI        18 \n",
            "03 ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏≠‡∏á Generative AI  \n",
            "Risks of Generative AI         23 \n",
            "3.1 ‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≥‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≥‡∏Å‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ  Generative AI    25 \n",
            "3.2 ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á‡∏Å‡∏≥‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á\n"
          ]
        }
      ],
      "source": [
        "# paths\n",
        "eng_path = \"/content/wiki_sample.txt\"\n",
        "thai_path = \"/content/20241125-Generative-AI-Guideline_V2-0.pdf\"\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\n",
        "if eng_path.lower().endswith(\".pdf\"):\n",
        "    eng_text = read_pdf(eng_path)\n",
        "else:\n",
        "    eng_text = read_text(eng_path)\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "if thai_path.lower().endswith(\".pdf\"):\n",
        "    thai_text = read_pdf(thai_path)\n",
        "else:\n",
        "    thai_text = read_text(thai_path)\n",
        "\n",
        "print(\"English text length:\", len(eng_text))\n",
        "print(eng_text[:800])\n",
        "\n",
        "print(\"\\n---\\nThai text length:\", len(thai_text))\n",
        "print(thai_text[:800])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BHvc6lykA1w",
        "outputId": "64b4b988-7a09-4625-824c-c7f8ee56b73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eng chunks: 7406\n",
            "Eng chunk[0]: The Project Gutenberg EBook of The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle (#15 in our series by Sir Arthur Conan Doyle) Copyright laws are changing all over the world. Be sure to check the copyright laws for your country before downloading or redistributing this or any other Project\n",
            "Thai chunks: 31\n",
            "Thai chunk[0]: 0 Generative AI Governance Guideline for Organizations 1 Generative AI Governance Guideline for Organizations ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ Executive Summary 4 01 ‡∏ó ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Generative AI What is Generative AI 7 1.1 ‡∏Ñ ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≥‡∏° 9 1.2 ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏´‡∏°‡∏≥‡∏¢ ‡∏Ç‡∏≠‡∏á Generative AI 10 1.3 ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏™‡∏≥‡∏°‡∏≥‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á Generative AI 12 02 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢\n"
          ]
        }
      ],
      "source": [
        "# English chunks\n",
        "eng_chunks = text_to_chunks_by_sentence(eng_text, max_words=200, overlap_words=40)\n",
        "print(\"Eng chunks:\", len(eng_chunks))\n",
        "print(\"Eng chunk[0]:\", eng_chunks[0][:300])\n",
        "\n",
        "# Thai chunks (light splitter)\n",
        "thai_chunks = text_to_chunks_light(thai_text, max_words=200, overlap_words=40)\n",
        "print(\"Thai chunks:\", len(thai_chunks))\n",
        "print(\"Thai chunk[0]:\", thai_chunks[0][:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxyMwbQ3mOqt",
        "outputId": "44cbdf16-5a9b-4d15-aec4-197ac2528b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedder ready\n",
            "Eng index size: 7406 Thai index size: 31\n"
          ]
        }
      ],
      "source": [
        "# load embedder\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedder ready\")\n",
        "\n",
        "# function to build index\n",
        "def build_faiss_index_from_chunks(chunks, embedder, batch_size=32):\n",
        "    vecs = []\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        em = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "        vecs.append(em)\n",
        "    vecs = np.vstack(vecs).astype('float32')\n",
        "    faiss.normalize_L2(vecs)\n",
        "    d = vecs.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(vecs)\n",
        "    return index, vecs\n",
        "\n",
        "# build for both\n",
        "eng_index, eng_vecs = build_faiss_index_from_chunks(eng_chunks, embed_model)\n",
        "thai_index, thai_vecs = build_faiss_index_from_chunks(thai_chunks, embed_model)\n",
        "print(\"Eng index size:\", eng_index.ntotal, \"Thai index size:\", thai_index.ntotal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OUIsJoAImTwv"
      },
      "outputs": [],
      "source": [
        "def retrieve(index, embedder, chunks, query, top_k=10):\n",
        "    if index.ntotal == 0:\n",
        "        return []\n",
        "    qvec = embedder.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    faiss.normalize_L2(qvec)\n",
        "    k = min(top_k, index.ntotal)\n",
        "    D, I = index.search(qvec, k)\n",
        "    results = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx < 0 or idx >= len(chunks):\n",
        "            continue\n",
        "        results.append((chunks[idx], float(score), int(idx)))\n",
        "    return results\n",
        "\n",
        "def mmr(query_embedding, doc_embeddings, docs, top_k=5, lambda_param=0.6):\n",
        "    sim_to_query = np.dot(doc_embeddings, query_embedding)\n",
        "    selected = []\n",
        "    not_selected = list(range(len(docs)))\n",
        "    if not not_selected:\n",
        "        return []\n",
        "    first = int(np.argmax(sim_to_query))\n",
        "    selected.append(first)\n",
        "    not_selected.remove(first)\n",
        "    while len(selected) < min(top_k, len(docs)):\n",
        "        mmr_score = []\n",
        "        for idx in not_selected:\n",
        "            relevance = sim_to_query[idx]\n",
        "            diversity = max(np.dot(doc_embeddings[idx], doc_embeddings[s]) for s in selected)\n",
        "            score = lambda_param * relevance - (1-lambda_param) * diversity\n",
        "            mmr_score.append((score, idx))\n",
        "        mmr_score.sort(reverse=True)\n",
        "        chosen = mmr_score[0][1]\n",
        "        selected.append(chosen)\n",
        "        not_selected.remove(chosen)\n",
        "    return selected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAvKVr2ImWrX",
        "outputId": "49c64692-e9d7-4d47-9ff6-cf170573e347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidates (eng):\n",
            "0 score 0.701 idx 1 preview: to make a donation to Project Gutenberg, and how to get involved. **Welcome To The World of Free Plain Vanilla Electronic Texts** **eBooks Readable By Both Humans and By Computers, Since 1971** *****T\n",
            "1 score 0.669 idx 471 preview: you go to the official police.\" \"Oh, I have heard of that fellow,\" answered my visitor, \"and I should be very glad if he would take the matter up, though of course I must use the official police as we\n",
            "2 score 0.636 idx 57 preview: Holmes. \"You have but to name it.\" \"This photograph!\" The King stared at him in amazement. \"Irene's photograph!\" he cried. \"Certainly, if you wish it.\" \"I thank your Majesty. Then there is no more to \n",
            "3 score 0.635 idx 342 preview: for some days.\" \"That was it,\" said Holmes, nodding approvingly; \"I have no doubt of it. But have you never been prosecuted for begging?\" \"Many times; but what was a fine to me?\" \"It must stop here, h\n",
            "4 score 0.615 idx 377 preview: town suppliers. Now, look at that third name. Just read it out to me.\" \"Mrs. Oakshott, 117, Brixton Road--249,\" read Holmes. \"Quite so. Now turn that up in the ledger.\" Holmes turned to the page indic\n",
            "5 score 0.599 idx 151 preview: photograph; but when I looked back to the weird business of the Sign of Four, and the extraordinary circumstances connected with the Study in Scarlet, I felt that it would be a strange tangle indeed w\n",
            "6 score 0.599 idx 231 preview: and records of the Sherlock Holmes cases between the years '82 and '90, I am faced by so many which present strange and interesting features that it is no easy matter to know which to choose and which\n",
            "7 score 0.598 idx 53 preview: pushed past the servant and rushed into the drawing-room, followed by the King and myself. The furniture was scattered about in every direction, with dismantled shelves and open drawers, as if the lad\n",
            "8 score 0.598 idx 168 preview: Station. Sherlock Holmes was pacing up and down the platform, his tall, gaunt figure made even gaunter and taller by his long grey travelling-cloak and close-fitting cloth cap. \"It is really very good\n",
            "9 score 0.596 idx 142 preview: Lyon Place, Camberwell.\" \"Mr. Angel's address you never had, I understand. Where is your father's place of business?\" \"He travels for Westhouse & Marbank, the great claret importers of Fenchurch Stree\n",
            "Selected (MMR): [1, 471, 231, 377]\n",
            "-> chunk 1 to make a donation to Project Gutenberg, and how to get involved. **Welcome To The World of Free Plain Vanilla Electronic Texts** **eBooks Readable By Both Humans and By Computers, Since 1971** *****These eBooks Were Prepared By Thousands of Volunteers! ***** Title: The Adventures of Sherlock Holmes\n",
            "-> chunk 471 you go to the official police.\" \"Oh, I have heard of that fellow,\" answered my visitor, \"and I should be very glad if he would take the matter up, though of course I must use the official police as well. Would you give me an introduction to him?\" \"I'll do better. I'll take you round to him myself.\" \n",
            "-> chunk 231 and records of the Sherlock Holmes cases between the years '82 and '90, I am faced by so many which present strange and interesting features that it is no easy matter to know which to choose and which to leave. Some, however, have already gained publicity through the papers, and others have not offe\n",
            "-> chunk 377 town suppliers. Now, look at that third name. Just read it out to me.\" \"Mrs. Oakshott, 117, Brixton Road--249,\" read Holmes. \"Quite so. Now turn that up in the ledger.\" Holmes turned to the page indicated. \"Here you are, 'Mrs. Oakshott, 117, Brixton Road, egg and poultry supplier.' \" \"Now, then, wha\n"
          ]
        }
      ],
      "source": [
        "query_eng = \"Can you tell me about The Adventures of Sherlock Holmes\"\n",
        "candidates = retrieve(eng_index, embed_model, eng_chunks, query_eng, top_k=10)\n",
        "print(\"Candidates (eng):\")\n",
        "for i,(txt,score,idx) in enumerate(candidates):\n",
        "    print(i, \"score\", round(score,3), \"idx\", idx, \"preview:\", txt[:200].replace(\"\\n\",\" \"))\n",
        "\n",
        "# MMR rerank (use eng_vecs)\n",
        "cand_indices = [c[2] for c in candidates]\n",
        "cand_embs = eng_vecs[cand_indices]\n",
        "qvec = embed_model.encode([query_eng], convert_to_numpy=True).astype('float32')\n",
        "# normalize\n",
        "from numpy.linalg import norm\n",
        "qvec = qvec[0] / (norm(qvec[0]) + 1e-12)\n",
        "sel_positions = mmr(qvec, cand_embs, [eng_chunks[i] for i in cand_indices], top_k=4, lambda_param=0.6)\n",
        "selected_indices = [cand_indices[pos] for pos in sel_positions]\n",
        "print(\"Selected (MMR):\", selected_indices)\n",
        "for idx in selected_indices:\n",
        "    print(\"-> chunk\", idx, eng_chunks[idx][:300].replace(\"\\n\",\" \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v96K-QoDtRN0",
        "outputId": "a4454960-2f02-4eee-a4d0-cb2b82acd7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidates (thai):\n",
            "0 score 0.37 idx 24 preview: ‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‚Ä¢ ‡∏†‡∏≥‡∏¢‡∏´‡∏•‡∏±‡∏á ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≥‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô (Use case) ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πâ‡∏≥‡∏´‡∏°‡∏≥‡∏¢‡πÉ‡∏ô‡∏Å‡∏≥ ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI ‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏Ñ‡∏ß‡∏£‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏£‡∏ô ‡∏≥ Generative AI ‡∏°‡∏≥‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≥‡∏∞\n",
            "1 score 0.369 idx 4 preview: ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡∏ã‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏Ñ‡πâ‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≥‡∏£‡∏™‡∏±‡πà‡∏á ‡∏Å‡∏≥‡∏£‡∏ú‡πà‡∏≥‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ ‡∏≥‡∏™‡∏±‡πà‡∏á ( Prompt) ‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Generative AI ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö Prompt ‡∏à‡∏≥‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô ‡πÅ‡∏•‡πâ‡∏ß Generative AI ‡∏à‡∏∞‡∏ó ‡∏≥ ‡∏Å‡∏≥‡∏£‡∏™‡∏£‡πâ‡∏≥‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≥‡πÉ‡∏´\n",
            "2 score 0.369 idx 13 preview: ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö ( Impact ) ‡∏ó‡∏µ‡πà‡∏≠‡∏≥‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏°‡∏¥‡∏ï‡∏¥‡∏ï‡πà‡∏≥‡∏á ‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏Ñ‡∏ß‡∏£‡∏´‡∏°‡∏±‡πà‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡∏≥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á Generative AI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏≥‡∏°‡∏≥‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£ ‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≥‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≥‡∏Å‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞\n",
            "3 score 0.362 idx 15 preview: ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏°‡∏≥‡∏ï‡∏£‡∏Å‡∏≥‡∏£ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ù‡πâ‡∏≥‡∏£‡∏∞‡∏ß‡∏±‡∏á ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ó‡∏≥‡∏á‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå (Establish Cyber Security Mechanisms ) ‡∏à‡∏±‡∏î‡∏ó ‡∏≥‡∏°‡∏≥‡∏ï‡∏£‡∏Å‡∏≥‡∏£ ‡∏Å‡∏≥‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≥‡∏Ñ‡∏ß‡∏≥‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏ó‡∏≥‡∏á‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏£‡πÄ‡∏Ç‡πâ‡∏≥‡∏ñ‡∏∂‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≥‡∏ï (Unau\n",
            "4 score 0.36 idx 11 preview: ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≥‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≥‡∏£‡∏ì‡πå‡∏ó‡∏≥ ‡∏á ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≥‡∏™‡∏ï‡∏£‡πå ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≥‡∏à‡∏ó ‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≥‡∏£‡∏ô ‡∏≥‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≥‡∏ó‡∏µ‡πà ‡∏ú‡∏¥‡∏î‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô 26 Generative AI Governance Guideline for Organizations 3) ‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏î‡πâ‡∏≥‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≥ ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≥‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á (Dang\n",
            "5 score 0.36 idx 10 preview: ‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡πà‡∏≥‡∏á ‡πÜ ‡∏Ç‡∏≠‡∏á Generative AI ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≥‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥ ‡∏†‡∏≥‡∏û 2.2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI 23 03 ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏≠‡∏á Generative AI Risks of Generative AI 24 Generative AI Governance Gu\n",
            "6 score 0.358 idx 26 preview: Data Leakage) ‚Ä¢ ‡∏Å‡∏≥‡∏£‡∏•‡∏∞‡πÄ‡∏°‡∏¥‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡∏ó‡∏≥‡∏á‡∏õ‡∏±‡∏ç‡∏ç‡∏≥ ( Intellectual Property Infringement) ‚Ä¢ ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ( Information Security) ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≥‡∏£‡∏™‡∏°‡∏Ñ‡∏ß‡∏£‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≥‡∏¢‡∏Å‡∏≥‡∏£ ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå ‡πÉ‡∏ä‡πâ Generative AI (Acceptab\n",
            "7 score 0.358 idx 14 preview: ‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á‡∏Å‡∏≥‡∏£‡∏ó ‡∏≥‡∏á‡∏≥‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≥‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÅ‡∏•‡∏∞ Generative AI (Ensure Human Oversight) ‡∏™‡∏£‡πâ‡∏≥‡∏á‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á‡∏Å‡∏≥‡∏£‡∏ó ‡∏≥‡∏á‡∏≥‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≥‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÅ‡∏•‡∏∞ Generative AI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≥‡∏£‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≥ Generative AI ‡∏°‡∏≥‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÇ‡∏î‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏° ‡∏ö‡∏ó\n",
            "8 score 0.356 idx 28 preview: ‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≥‡∏ô‡πÄ‡∏ó‡πà‡∏≥‡∏ô‡∏±‡πâ‡∏ô 57 Generative AI Governance Guideline for Organizations 2) ‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≥‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏≥‡∏ó‡∏≠‡∏≥‡∏à‡∏ó ‡∏≥‡πÉ‡∏´‡πâ‡∏™ ‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≥‡∏ô ‡∏Å‡∏£‡∏∞‡∏ó ‡∏≥‡∏Å‡∏≥‡∏£‡∏•‡∏∞‡πÄ‡∏°‡∏¥‡∏î‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÉ‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡∏ó‡∏≥‡∏á‡∏õ‡∏±‡∏ç‡∏ç‡∏≥‡πÑ‡∏î‡πâ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô Ge\n",
            "9 score 0.354 idx 2 preview: ‡∏Å‡∏≥‡∏£‡∏™‡∏£‡πâ‡∏≥‡∏á‡∏Ñ‡∏ß‡∏≥‡∏° ‡πÄ‡∏Ç‡πâ‡∏≥‡πÉ‡∏à‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏≠‡∏á Generative AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á ‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£‡∏≠‡∏¢‡πà‡∏≥‡∏á ‡πÄ‡∏´‡∏°‡∏≥‡∏∞‡∏™‡∏°‡∏ï‡∏≥‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡∏Å‡∏≥‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô ‡∏à‡∏£‡∏¥‡∏á 4 ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á‡∏Å‡∏≥‡∏£‡∏ô ‡∏≥ Generative AI ‡∏°‡∏≥‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ - ‡∏Å‡∏≥‡∏£‡∏™‡∏£‡πâ‡∏≥‡∏á‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏Ç‡πâ‡∏≥‡πÉ‡∏à ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≥‡∏°\n",
            "Selected (MMR): [24, 15, 13, 10]\n",
            "-> chunk 24 ‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‚Ä¢ ‡∏†‡∏≥‡∏¢‡∏´‡∏•‡∏±‡∏á ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≥‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô (Use case) ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πâ‡∏≥‡∏´‡∏°‡∏≥‡∏¢‡πÉ‡∏ô‡∏Å‡∏≥ ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI ‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏Ñ‡∏ß‡∏£‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏£‡∏ô ‡∏≥ Generative AI ‡∏°‡∏≥‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≥‡∏∞‡∏™‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Off- the-Shelf ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ï‡∏•‡∏≥‡∏î ‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ RAG ‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ ‡∏Å‡∏≥‡∏£ Fine\n",
            "-> chunk 15 ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏°‡∏≥‡∏ï‡∏£‡∏Å‡∏≥‡∏£ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ù‡πâ‡∏≥‡∏£‡∏∞‡∏ß‡∏±‡∏á ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ó‡∏≥‡∏á‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå (Establish Cyber Security Mechanisms ) ‡∏à‡∏±‡∏î‡∏ó ‡∏≥‡∏°‡∏≥‡∏ï‡∏£‡∏Å‡∏≥‡∏£ ‡∏Å‡∏≥‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≥‡∏Ñ‡∏ß‡∏≥‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏ó‡∏≥‡∏á‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≥‡∏£‡πÄ‡∏Ç‡πâ‡∏≥‡∏ñ‡∏∂‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≥‡∏ï (Unauthorized Access) ‡∏Å‡∏≥‡∏£‡πÄ‡∏à‡∏≥‡∏∞‡∏£‡∏∞‡∏ö‡∏ö (Hacking) ‡∏Å‡∏≥‡∏£‡∏•‡∏∞‡πÄ‡∏°‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Breaches) ‡∏Å‡∏≥‡∏£‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏´‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ( Data Leaka\n",
            "-> chunk 13 ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö ( Impact ) ‡∏ó‡∏µ‡πà‡∏≠‡∏≥‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏°‡∏¥‡∏ï‡∏¥‡∏ï‡πà‡∏≥‡∏á ‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏Ñ‡∏ß‡∏£‡∏´‡∏°‡∏±‡πà‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡∏≥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á Generative AI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏≥‡∏°‡∏≥‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≥‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£ ‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≥‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≥‡∏Å‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI ‡∏≠‡∏¢‡πà‡∏≥‡∏á‡πÄ‡∏´‡∏°‡∏≥‡∏∞‡∏™‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ 3.2 ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á Generative AI ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≥‡∏°‡∏™‡∏≥‡∏°\n",
            "-> chunk 10 ‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡πà‡∏≥‡∏á ‡πÜ ‡∏Ç‡∏≠‡∏á Generative AI ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≥‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥ ‡∏†‡∏≥‡∏û 2.2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI 23 03 ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏≠‡∏á Generative AI Risks of Generative AI 24 Generative AI Governance Guideline for Organizations ‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≥‡∏û‡∏ô‡∏±‡πâ‡∏ô ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£‡πÅ‡∏•‡∏∞ ‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢\n"
          ]
        }
      ],
      "source": [
        "query_th = \"‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á AI ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "candidates = retrieve(thai_index, embed_model, thai_chunks, query_th, top_k=10)\n",
        "print(\"Candidates (thai):\")\n",
        "for i,(txt,score,idx) in enumerate(candidates):\n",
        "    print(i, \"score\", round(score,3), \"idx\", idx, \"preview:\", txt[:200].replace(\"\\n\",\" \"))\n",
        "\n",
        "cand_indices = [c[2] for c in candidates]\n",
        "cand_embs = thai_vecs[cand_indices]\n",
        "qvec = embed_model.encode([query_th], convert_to_numpy=True).astype('float32')\n",
        "qvec = qvec[0] / (np.linalg.norm(qvec[0]) + 1e-12)\n",
        "sel_positions = mmr(qvec, cand_embs, [thai_chunks[i] for i in cand_indices], top_k=4, lambda_param=0.6)\n",
        "selected_indices = [cand_indices[pos] for pos in sel_positions]\n",
        "print(\"Selected (MMR):\", selected_indices)\n",
        "for idx in selected_indices:\n",
        "    print(\"-> chunk\", idx, thai_chunks[idx][:300].replace(\"\\n\",\" \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "bd9598853ff640daabd3c76c15b946cc",
            "03c72c5f186a40438a487bcb306421ec",
            "cb22d74142ef4e0992bc7b50e469f502",
            "c2276483430e445d96075d3cea0425a9",
            "e17ab0dae0684df78df2b67669ca4354",
            "4a06191c74044b9aaa5e06baff8df6b4",
            "9bf20b31f8bc4efdb922c6132e1c9d96",
            "5762758643de41259d088cfaf468762a",
            "86d62337770c4890a86d269fd509d4b5",
            "1cdee31b39a54c77ac6639bccf13ec5f",
            "78acd2049c5e4f818f7a41d7bdb468c4"
          ]
        },
        "id": "7XdWn7jvtZfF",
        "outputId": "1e255d7f-ad47-4d57-8742-a68b01a4b97e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd9598853ff640daabd3c76c15b946cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type: causal\n"
          ]
        }
      ],
      "source": [
        "# loader (safe)\n",
        "FALLBACK_SEQ2SEQ = \"google/flan-t5-small\"\n",
        "\n",
        "def load_llm(model_id=\"google/gemma-2b-it\", token=None):\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "        m = AutoModelForCausalLM.from_pretrained(model_id, token=token)\n",
        "        if getattr(m.config, 'pad_token_id', None) is None:\n",
        "            m.config.pad_token_id = m.config.eos_token_id\n",
        "            tok.pad_token = tok.eos_token\n",
        "        m.to(device)\n",
        "        return tok, m, 'causal'\n",
        "    except Exception as e:\n",
        "        print(\"Causal load failed:\", e)\n",
        "    # fallback\n",
        "    tok = AutoTokenizer.from_pretrained(FALLBACK_SEQ2SEQ)\n",
        "    m = AutoModelForSeq2SeqLM.from_pretrained(FALLBACK_SEQ2SEQ)\n",
        "    if getattr(m.config, 'pad_token_id', None) is None:\n",
        "        m.config.pad_token_id = m.config.eos_token_id\n",
        "        tok.pad_token = tok.eos_token\n",
        "    m.to(device)\n",
        "    return tok, m, 'seq2seq'\n",
        "\n",
        "token = os.environ.get('HF_TOKEN', None)  # or None if not logged in\n",
        "tokenizer, model, model_type = load_llm(token=token)\n",
        "print(\"Model type:\", model_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV0qebfp9KLF",
        "outputId": "218eb43a-ec80-4fa3-c4c4-2d6c58ec3bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT (eng) preview:\\n You are an expert Story teller. Use the following context to answer the question. If the answer is not in the context, say 'I don't know'.\\n\\nContext:\\n[chunk 24]\\n\"Was the photograph a cabinet?\" \"It was.\" \"Then, good-night, your Majesty, and I trust that we shall soon have some good news for you. And good-night, Watson,\" he added, as the wheels of the royal brougham rolled down the street. \"If you will be good enough to call to-morrow afternoon at three o'clock I should like to chat this little matter over with you.\" II. At three o'clock precisely I was at Baker Street, but Holmes had not yet returned. The landlady informed me that he had left the house shortly after eight o'clock in the morning. I sat down beside the fire, however, with the intention of awaiting him, however long he migh\n",
            "=== Answer (eng) ===\\n I cannot provide a summary of The Adventures of Sherlock Holmes from the context, as the context does not provide any information about the book.\n"
          ]
        }
      ],
      "source": [
        "def build_rag_prompt_from_indices(question, chunks, indices, role='story teller', max_context_chars=2000):\n",
        "    context = ''\n",
        "    for idx in indices:\n",
        "        context += f\"[chunk {idx}]\\\\n\" + chunks[idx].strip() + '\\\\n\\\\n---\\\\n\\\\n'\n",
        "        if len(context) > max_context_chars:\n",
        "            break\n",
        "    prompt = f\"You are an expert {role}. Use the following context to answer the question. If the answer is not in the context, say 'I don't know'.\\\\n\\\\nContext:\\\\n{context}\\\\nQuestion: {question}\\\\n\\\\nAnswer concisely.\"\n",
        "    return prompt\n",
        "\n",
        "# Build prompt for English\n",
        "prompt_eng = build_rag_prompt_from_indices(query_eng, eng_chunks, selected_indices, role='Story teller')\n",
        "print(\"PROMPT (eng) preview:\\\\n\", prompt_eng[:800])\n",
        "\n",
        "# Generate\n",
        "def generate_from_llm_prompt(prompt, tokenizer, model, model_type='causal', max_new_tokens=150):\n",
        "    if model_type == 'causal':\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.2, top_p=0.95)\n",
        "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        return text[len(prompt):].strip() if text.startswith(prompt) else text\n",
        "    else:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        return text\n",
        "\n",
        "ans_eng = generate_from_llm_prompt(prompt_eng, tokenizer, model, model_type=model_type, max_new_tokens=150)\n",
        "print(\"=== Answer (eng) ===\\\\n\", ans_eng)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7QxHkwm9MMv",
        "outputId": "ba57a9a1-e4f6-4527-f87b-f927b016d3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT (thai) preview:\\n You are an expert ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô AI. Use the following context to answer the question. If the answer is not in the context, say 'I don't know'.\\n\\nContext:\\n[chunk 24]\\n‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≥‡∏£‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‚Ä¢ ‡∏†‡∏≥‡∏¢‡∏´‡∏•‡∏±‡∏á ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≥‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≥‡∏ô (Use case) ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πâ‡∏≥‡∏´‡∏°‡∏≥‡∏¢‡πÉ‡∏ô‡∏Å‡∏≥ ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Generative AI ‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏Ñ‡∏ß‡∏£‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏£‡∏ô ‡∏≥ Generative AI ‡∏°‡∏≥‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≥‡∏∞‡∏™‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Off- the-Shelf ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ï‡∏•‡∏≥‡∏î ‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ RAG ‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ ‡∏Å‡∏≥‡∏£ Fine-Tuning ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≥‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó ‡∏≥ Pre-training ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô ‚Ä¢ ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Å‡∏≥‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á ‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏°‡∏≥‡∏ï‡∏£‡∏Å‡∏≥‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≥‡∏£ ‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Generative AI (‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏•‡πà‡∏≥‡∏ß‡πÑ‡∏ß‡πâ ‡πÉ‡∏ô‡∏ö‡∏ó‡∏ó‡∏µ‡πà 3) ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ (Risk Appetite) ‚Ä¢ ‡∏Ñ‡∏ß‡∏£‡∏Å ‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≥‡∏£‡∏ó ‡∏≥‡∏á‡∏≥‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≥‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÅ‡∏•‡∏∞ Generative AI ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á ‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≥‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö ‡πÄ‡∏ä‡∏¥\n",
            "=== Answer (thai) ===\\n The context does not specify what the elements of AI are, so I cannot answer this question from the context.\n"
          ]
        }
      ],
      "source": [
        "prompt_th = build_rag_prompt_from_indices(query_th, thai_chunks, selected_indices, role='‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô AI')\n",
        "print(\"PROMPT (thai) preview:\\\\n\", prompt_th[:800])\n",
        "\n",
        "ans_th = generate_from_llm_prompt(prompt_th, tokenizer, model, model_type=model_type, max_new_tokens=150)\n",
        "print(\"=== Answer (thai) ===\\\\n\", ans_th)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Module 3.2 ‚Äî Retrieval Improvement (One-Cell Version)\n",
        "# ============================\n",
        "import numpy as np\n",
        "\n",
        "# ---- Auto Top-K ----\n",
        "def auto_top_k(num_chunks):\n",
        "    k = int(np.log2(num_chunks) * 2)\n",
        "    return max(5, min(k, 50))  # ‡πÑ‡∏°‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 5 ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 50\n",
        "\n",
        "# ---- Similarity Threshold ----\n",
        "def filter_by_similarity(results, threshold=0.2):\n",
        "    # results: [(text, score, idx)]\n",
        "    return [r for r in results if r[1] >= threshold]\n",
        "\n",
        "# ---- Expand Query ----\n",
        "def expand_queries(query):\n",
        "    return [\n",
        "        query,\n",
        "        query.lower(),\n",
        "        query.replace(\"what\", \"\").strip(),\n",
        "        \"Explain \" + query,\n",
        "        query + \" details\"\n",
        "    ]\n",
        "\n",
        "# ---- Multi-Query Retrieval ----\n",
        "def multi_query_retrieve(query, index, embed_model, chunks, top_k):\n",
        "    qs = expand_queries(query)\n",
        "    all_results = []\n",
        "    for q in qs:\n",
        "        res = retrieve(index, embed_model, chunks, q, top_k)\n",
        "        all_results.extend(res)\n",
        "\n",
        "    # merge unique idx\n",
        "    unique = {}\n",
        "    for txt, score, idx in all_results:\n",
        "        if idx not in unique or score > unique[idx][1]:\n",
        "            unique[idx] = (txt, score)\n",
        "\n",
        "    merged = [(t, s, i) for i, (t, s) in unique.items()]\n",
        "    merged.sort(key=lambda x: x[1], reverse=True)\n",
        "    return merged\n",
        "\n",
        "# ---- MMR Rerank ----\n",
        "def mmr_rerank(query, results, embed_model, chunks, top_k=5, lambda_param=0.5):\n",
        "    if len(results) == 0:\n",
        "        return []\n",
        "\n",
        "    # embed query\n",
        "    query_vec = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "\n",
        "    # embed candidate chunks\n",
        "    cand_texts = [r[0] for r in results]\n",
        "    cand_vecs = embed_model.encode(cand_texts, convert_to_numpy=True).astype(\"float32\")\n",
        "\n",
        "    selected = []\n",
        "    not_selected = list(range(len(cand_texts)))\n",
        "\n",
        "    # pick highest similarity first\n",
        "    first = int(np.argmax(np.dot(cand_vecs, query_vec.T)))\n",
        "    selected.append(first)\n",
        "    not_selected.remove(first)\n",
        "\n",
        "    while len(selected) < min(top_k, len(cand_texts)):\n",
        "        mmr_scores = []\n",
        "        for i in not_selected:\n",
        "            relevance = np.dot(cand_vecs[i], query_vec.T)\n",
        "            diversity = max(np.dot(cand_vecs[i], cand_vecs[j]) for j in selected)\n",
        "            score = lambda_param * relevance - (1 - lambda_param) * diversity\n",
        "            mmr_scores.append((score, i))\n",
        "\n",
        "        mmr_scores.sort(reverse=True)\n",
        "        chosen = mmr_scores[0][1]\n",
        "        selected.append(chosen)\n",
        "        not_selected.remove(chosen)\n",
        "\n",
        "    return [results[i][2] for i in selected]  # return chunk indices\n",
        "\n",
        "print(\"Module 3.2 loaded successfully! üöÄ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBc45lrfcOha",
        "outputId": "428b7c77-c451-4394-eecb-9ce55dc9b130"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module 3.2 loaded successfully! üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TgOQnou1K_9",
        "outputId": "9ef29757-8e33-4081-c88a-76fc1300be17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected chunk indices: [168, 396, 175, 2, 397]\n"
          ]
        }
      ],
      "source": [
        "query = \"What cases did Sherlock Holmes investigate?\"\n",
        "\n",
        "top_k = auto_top_k(len(eng_chunks))\n",
        "\n",
        "# multi-query\n",
        "raw_results = multi_query_retrieve(query, index, embed_model, eng_chunks, top_k)\n",
        "\n",
        "# filter low similarity\n",
        "filtered = filter_by_similarity(raw_results, threshold=0.2)\n",
        "\n",
        "# MMR rerank\n",
        "selected_indices = mmr_rerank(query, filtered, embed_model, eng_chunks, top_k=5)\n",
        "\n",
        "print(\"Selected chunk indices:\", selected_indices)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f02eb1f1a3fd4084bb73855e171ef1c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03058aaf087b4d9e80112629f83da4f0",
              "IPY_MODEL_01c7f50e59cb4e25bde67080aa100544",
              "IPY_MODEL_b9b435728c1d4e0da388cce2a57ed0b7",
              "IPY_MODEL_aadd71386f7942b4bb98c7d4516122b0",
              "IPY_MODEL_25ed4ba7f75d48d0b0fe0ca1ee2ce910"
            ],
            "layout": "IPY_MODEL_91ae609281274d8ca9cb394a37ffff15"
          }
        },
        "03058aaf087b4d9e80112629f83da4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_224610ccc6f240ae9f3d04a03c904d85",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b97fade6aaa94fefa3241f998d3e880b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "01c7f50e59cb4e25bde67080aa100544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_65ec0edee62c49588af67157c7aa9bd5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c77d7ad514df437b89d9294bdfd768c0",
            "value": ""
          }
        },
        "b9b435728c1d4e0da388cce2a57ed0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_890286cf93164347b8c14dbdbc43a7ef",
            "style": "IPY_MODEL_295765149a24496a9408090b1e247ead",
            "value": true
          }
        },
        "aadd71386f7942b4bb98c7d4516122b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cdb7eeba3e7944d7be426b74e7bbdbb0",
            "style": "IPY_MODEL_f35779a8343c43e6b6547979b1d7a41f",
            "tooltip": ""
          }
        },
        "25ed4ba7f75d48d0b0fe0ca1ee2ce910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe4b92a65d3419ba22fcdce9c0ff179",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b87cce326878451dac31913788cc6d4c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "91ae609281274d8ca9cb394a37ffff15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "224610ccc6f240ae9f3d04a03c904d85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97fade6aaa94fefa3241f998d3e880b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65ec0edee62c49588af67157c7aa9bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77d7ad514df437b89d9294bdfd768c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "890286cf93164347b8c14dbdbc43a7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "295765149a24496a9408090b1e247ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdb7eeba3e7944d7be426b74e7bbdbb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35779a8343c43e6b6547979b1d7a41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "dbe4b92a65d3419ba22fcdce9c0ff179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87cce326878451dac31913788cc6d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd9598853ff640daabd3c76c15b946cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03c72c5f186a40438a487bcb306421ec",
              "IPY_MODEL_cb22d74142ef4e0992bc7b50e469f502",
              "IPY_MODEL_c2276483430e445d96075d3cea0425a9"
            ],
            "layout": "IPY_MODEL_e17ab0dae0684df78df2b67669ca4354"
          }
        },
        "03c72c5f186a40438a487bcb306421ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a06191c74044b9aaa5e06baff8df6b4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9bf20b31f8bc4efdb922c6132e1c9d96",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "cb22d74142ef4e0992bc7b50e469f502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5762758643de41259d088cfaf468762a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86d62337770c4890a86d269fd509d4b5",
            "value": 2
          }
        },
        "c2276483430e445d96075d3cea0425a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cdee31b39a54c77ac6639bccf13ec5f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_78acd2049c5e4f818f7a41d7bdb468c4",
            "value": "‚Äá2/2‚Äá[00:34&lt;00:00,‚Äá14.36s/it]"
          }
        },
        "e17ab0dae0684df78df2b67669ca4354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a06191c74044b9aaa5e06baff8df6b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bf20b31f8bc4efdb922c6132e1c9d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5762758643de41259d088cfaf468762a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d62337770c4890a86d269fd509d4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cdee31b39a54c77ac6639bccf13ec5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78acd2049c5e4f818f7a41d7bdb468c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}